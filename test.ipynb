{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = os.path.join(os.getcwd(), 'cache')\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cd9bef40ea43c6aeb70a828b6cf051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" You asked for it a whole video devoted to the rainbow The army arts and crafts department sent me to Korea now not a normal This is so dense three is because I can't stand just one Another thing that I want to talk about is The hygiene of I squeezed and squeezed and squeezed and squeezed No flap on this arm from my cleaning\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "API_TOKEN = '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'\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openai/whisper-tiny\"\n",
    "\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "data = query(\"test.mp3\")\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration fr-FR-845eca745fd02f1b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset minds14/fr-FR to C:/Users/likit/.cache/huggingface/datasets/PolyAI___minds14/fr-FR-845eca745fd02f1b/1.0.0/aa40414f15e0f919231d617440192034af844835dc1e6a697f4b552e0551fd26...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach http://poly-public-data.s3.amazonaws.com/MInDS-14/MInDS-14.zip (ProxyError(MaxRetryError(\"HTTPConnectionPool(host='127.0.0.1', port=8888): Max retries exceeded with url: http://poly-public-data.s3.amazonaws.com/MInDS-14/MInDS-14.zip (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000018B56530FD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')))\")))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\likit\\Desktop\\asr\\test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/likit/Desktop/asr/test.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/likit/Desktop/asr/test.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m minds_14 \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mPolyAI/minds14\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfr-FR\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_dir\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetcwd(), \u001b[39m'\u001b[39;49m\u001b[39mcache\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mMInDS-14\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\load.py:1741\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1740\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   1742\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1743\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1744\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[0;32m   1745\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   1746\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1747\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   1748\u001b[0m )\n\u001b[0;32m   1750\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   1752\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   1753\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\builder.py:822\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    821\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[1;32m--> 822\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    823\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[0;32m    824\u001b[0m         verify_infos\u001b[39m=\u001b[39mverify_infos,\n\u001b[0;32m    825\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    826\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    827\u001b[0m     )\n\u001b[0;32m    828\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    829\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\builder.py:1555\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verify_infos, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verify_infos, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1556\u001b[0m         dl_manager, verify_infos, check_duplicate_keys\u001b[39m=\u001b[39mverify_infos, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs\n\u001b[0;32m   1557\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\builder.py:891\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m    890\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[1;32m--> 891\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_generators(dl_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msplit_generators_kwargs)\n\u001b[0;32m    893\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[39mif\u001b[39;00m verify_infos \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\PolyAI--minds14\\aa40414f15e0f919231d617440192034af844835dc1e6a697f4b552e0551fd26\\minds14.py:132\u001b[0m, in \u001b[0;36mMinds14._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split_generators\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[0;32m    126\u001b[0m     langs \u001b[39m=\u001b[39m (\n\u001b[0;32m    127\u001b[0m         _ALL_CONFIGS\n\u001b[0;32m    128\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m         \u001b[39melse\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mname]\n\u001b[0;32m    130\u001b[0m     )\n\u001b[1;32m--> 132\u001b[0m     archive_path \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload_and_extract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdata_url)\n\u001b[0;32m    133\u001b[0m     audio_path \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mextract(\n\u001b[0;32m    134\u001b[0m         os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(archive_path, \u001b[39m\"\u001b[39m\u001b[39mMInDS-14\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maudio.zip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m     text_path \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mextract(\n\u001b[0;32m    137\u001b[0m         os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(archive_path, \u001b[39m\"\u001b[39m\u001b[39mMInDS-14\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtext.zip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\download\\download_manager.py:447\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_and_extract\u001b[39m(\u001b[39mself\u001b[39m, url_or_urls):\n\u001b[0;32m    432\u001b[0m     \u001b[39m\"\"\"Download and extract given url_or_urls.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \n\u001b[0;32m    434\u001b[0m \u001b[39m    Is roughly equivalent to:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[39m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload(url_or_urls))\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\download\\download_manager.py:311\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    308\u001b[0m download_func \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download, download_config\u001b[39m=\u001b[39mdownload_config)\n\u001b[0;32m    310\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m--> 311\u001b[0m downloaded_path_or_paths \u001b[39m=\u001b[39m map_nested(\n\u001b[0;32m    312\u001b[0m     download_func,\n\u001b[0;32m    313\u001b[0m     url_or_urls,\n\u001b[0;32m    314\u001b[0m     map_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    315\u001b[0m     num_proc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[0;32m    316\u001b[0m     parallel_min_length\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[0;32m    317\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n\u001b[0;32m    318\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    319\u001b[0m )\n\u001b[0;32m    320\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    321\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading took \u001b[39m\u001b[39m{\u001b[39;00mduration\u001b[39m.\u001b[39mtotal_seconds() \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m60\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\utils\\py_utils.py:436\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39m# Singleton\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n\u001b[1;32m--> 436\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n\u001b[0;32m    438\u001b[0m disable_tqdm \u001b[39m=\u001b[39m disable_tqdm \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m    439\u001b[0m iterable \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data_struct\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m data_struct\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\download\\download_manager.py:338\u001b[0m, in \u001b[0;36mDownloadManager._download\u001b[1;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[39mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[0;32m    336\u001b[0m     \u001b[39m# append the relative path to the base_path\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     url_or_filename \u001b[39m=\u001b[39m url_or_path_join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_path, url_or_filename)\n\u001b[1;32m--> 338\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(url_or_filename, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\utils\\file_utils.py:183\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[0;32m    181\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    182\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[0;32m    184\u001b[0m         url_or_filename,\n\u001b[0;32m    185\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    186\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[0;32m    187\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[0;32m    188\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[0;32m    189\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[0;32m    190\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[0;32m    191\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[0;32m    192\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    193\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[0;32m    194\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[0;32m    195\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    198\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\utils\\file_utils.py:533\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\u001b[0m\n\u001b[0;32m    531\u001b[0m _raise_if_offline_mode_is_enabled(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTried to reach \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m head_error \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt reach \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(head_error)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[39melif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt reach \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m (error \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mConnectionError\u001b[0m: Couldn't reach http://poly-public-data.s3.amazonaws.com/MInDS-14/MInDS-14.zip (ProxyError(MaxRetryError(\"HTTPConnectionPool(host='127.0.0.1', port=8888): Max retries exceeded with url: http://poly-public-data.s3.amazonaws.com/MInDS-14/MInDS-14.zip (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000018B56530FD0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')))\")))"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "minds_14 = load_dataset(\"PolyAI/minds14\", \"fr-FR\", data_dir=os.path.join(os.getcwd(), 'cache', 'MInDS-14'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015148639678955078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading builder script",
       "rate": null,
       "total": 7480,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08986f79e5e44cb81b4b403ee3cead0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016138553619384766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading readme",
       "rate": null,
       "total": 11130,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6082e846e24f4435b59eaa9f66642ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset timit_asr/clean to c:/Users/likit/Desktop/asr/cache/datasets/timit_asr/clean/2.0.1/43f9448dd5db58e95ee48a277f466481b151f112ea53e27f8173784da9254fb2...\n"
     ]
    },
    {
     "ename": "ManualDownloadError",
     "evalue": "                    The dataset timit_asr with config clean requires manual data.\n                    Please follow the manual download instructions:\n                     To use TIMIT you have to download it manually. Please create an account and download the dataset from https://catalog.ldc.upenn.edu/LDC93S1 \nThen extract all files in one folder and load the dataset with: `datasets.load_dataset('timit_asr', data_dir='path/to/folder/folder_name')`\n                    Manual data can be loaded with:\n                     datasets.load_dataset(\"timit_asr\", data_dir=\"<path/to/manual/data>\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mManualDownloadError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\likit\\Desktop\\asr\\test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/likit/Desktop/asr/test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset, Audio, load_metric\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/likit/Desktop/asr/test.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m WhisperConfig, WhisperModel\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/likit/Desktop/asr/test.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mtimit_asr\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/likit/Desktop/asr/test.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\load.py:1741\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1740\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   1742\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1743\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1744\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[0;32m   1745\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   1746\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1747\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   1748\u001b[0m )\n\u001b[0;32m   1750\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   1752\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   1753\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\builder.py:798\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    793\u001b[0m     _dest \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs\u001b[39m.\u001b[39m_strip_protocol(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir) \u001b[39mif\u001b[39;00m is_local \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir\n\u001b[0;32m    794\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    795\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading and preparing dataset \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mbuilder_name\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mconfig_name\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00m_dest\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m     )\n\u001b[1;32m--> 798\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_manual_download(dl_manager)\n\u001b[0;32m    800\u001b[0m \u001b[39m# Create a tmp dir and rename to self._output_dir on successful exit.\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39mwith\u001b[39;00m incomplete_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir) \u001b[39mas\u001b[39;00m tmp_output_dir:\n\u001b[0;32m    802\u001b[0m     \u001b[39m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001b[39;00m\n\u001b[0;32m    803\u001b[0m     \u001b[39m# it to every sub function.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\likit\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\datasets\\builder.py:845\u001b[0m, in \u001b[0;36mDatasetBuilder._check_manual_download\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_manual_download\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[0;32m    844\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mmanual_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 845\u001b[0m         \u001b[39mraise\u001b[39;00m ManualDownloadError(\n\u001b[0;32m    846\u001b[0m             textwrap\u001b[39m.\u001b[39mdedent(\n\u001b[0;32m    847\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[39m                The dataset \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m with config \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m requires manual data.\u001b[39m\n\u001b[0;32m    849\u001b[0m \u001b[39m                Please follow the manual download instructions:\u001b[39m\n\u001b[0;32m    850\u001b[0m \u001b[39m                 \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions\u001b[39m}\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[39m                Manual data can be loaded with:\u001b[39m\n\u001b[0;32m    852\u001b[0m \u001b[39m                 datasets.load_dataset(\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, data_dir=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<path/to/manual/data>\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    853\u001b[0m             )\n\u001b[0;32m    854\u001b[0m         )\n",
      "\u001b[1;31mManualDownloadError\u001b[0m:                     The dataset timit_asr with config clean requires manual data.\n                    Please follow the manual download instructions:\n                     To use TIMIT you have to download it manually. Please create an account and download the dataset from https://catalog.ldc.upenn.edu/LDC93S1 \nThen extract all files in one folder and load the dataset with: `datasets.load_dataset('timit_asr', data_dir='path/to/folder/folder_name')`\n                    Manual data can be loaded with:\n                     datasets.load_dataset(\"timit_asr\", data_dir=\"<path/to/manual/data>\")"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio, load_metric\n",
    "from transformers import WhisperConfig, WhisperModel\n",
    "\n",
    "data = load_dataset(\"timit_asr\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2282eb58c9f8b7eb2a5559c7507abe43781dffd7b73acc17606c3e0dd7a35299"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
